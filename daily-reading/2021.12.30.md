### \[2019\] Invariant Risk Minimization
> a correlation is spurious when we do not expect it to hold in the future in the same manner as it held in the past

> To learn invariances across environments, find a data representation such that the optimal classifier on top of that representation matches for all environments.

#### how to learn invariant and causal regresion?
1. Empirical Risk Minimization (EMR). Merge all data and optimize on the training error.
2. Robust learning. Minimize the maximum environment risk.
    > it turns out to be equivalent to minimizing a weighted average of environment training errors.
    
    > EMR is a special case of robust learning.
    
3. Domain adaptation. Estimate a data representation $\Phi (X_1, X_2)$ that follows same distribution for all environments. (could fail)
4. Invariant causal prediction. Search for subset of variables that produce regression residuals with equal distribution across all environments.

IRM aims to find a data representation, so that the optimal classifier performs same for all environments.

### Algorithm

$y = w * \Phi (x)$. $\Phi$是我们要找的invariant表示，w是一个线性分类器。invariant的表示会导致一个invariant的predictor。相比于传统ML在所有环境上寻找优化的$\Phi$，现在对这个$\Phi$增加了限制，必须让基于它优化出的$w$在所有环境中保持不变$^1$。在条件$1$满足的前提下去寻找最优的$\Phi$。这是definition 3讲的东西。

以上是硬性的限制，然而实际很难实现。于是可以将条件$1$转为软限制 soft restriction.引入$D(w,\Phi,e)$去衡量不同环境下w的相似度。

再简化：固定w。$w=(1,0,...,0)$.

#