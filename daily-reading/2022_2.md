## \[2020-NIPS\] Learning from Failure: Training Debiased Classifier from Biased Classifier
2.25  
LfF
### Idea
- spurious correlation is easier to learn than desired knowledge
- biased model can learn such spurious correlation first and filter out bias-aligned and bias-conflicting samples 
- bias-conflicting samples are **up-weighted** to train a debias model

### Pick up
**GCE** generalized cross entropy
> amplify the bias by up-weighting the gradient of samples with high probability (correctly classified)

### Citation
Example of spurious correlations on object classification:
- "why should i trust you?" explaining the predictions of any classifier. SIGKDD 2016.
- Object recognition with and without objects. IJCAI 2017.

Training dynamics of NN: easy first, hard later
- A closer look at memorization in deep networks. ICML 2017.
- Robust inference via generative classifiers for handling noisy labels. ICML 2019.
- Insights on representational similarity in neural networks
with canonical correlation. NIPS 2018.

### Commonts
When bias ratio (ratio of bias-aligned samples) is extreamly high (95%+), the model outperform baseline models.
